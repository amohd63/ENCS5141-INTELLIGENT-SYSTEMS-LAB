{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNaRlcSo5lNzPKzCJnkSZOI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amohd63/ENCS5141-INTELLIGENT-SYSTEMS-LAB/blob/main/Manual_ENCS5141_Exp7_Introduction_to_Deep_Learning_with_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iyLt5zAl0fhK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensor Initialization"
      ],
      "metadata": {
        "id": "EPsyZ4qL0qRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [[1, 2], [3, 4]]\n",
        "x_data = torch.tensor(data)"
      ],
      "metadata": {
        "id": "jLnbZ0u10ym2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np_array = np.array(data)\n",
        "x_np = torch.from_numpy(np_array)"
      ],
      "metadata": {
        "id": "2oRU9BTM00o6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
        "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
        "\n",
        "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
        "print(f\"Random Tensor: \\n {x_rand} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xP4rvzeA02HQ",
        "outputId": "32334638-5508-4e76-82ec-98a6571a4069"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ones Tensor: \n",
            " tensor([[1, 1],\n",
            "        [1, 1]]) \n",
            "\n",
            "Random Tensor: \n",
            " tensor([[0.4619, 0.2755],\n",
            "        [0.0763, 0.2951]]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shape = (2, 3,)\n",
        "rand_tensor = torch.rand(shape)\n",
        "ones_tensor = torch.ones(shape)\n",
        "zeros_tensor = torch.zeros(shape)\n",
        "\n",
        "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
        "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
        "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05FSRIUB07vd",
        "outputId": "f4472426-d651-4641-cb2d-399ec4b0bf99"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Tensor: \n",
            " tensor([[0.1292, 0.2699, 0.7127],\n",
            "        [0.6337, 0.6406, 0.5800]]) \n",
            "\n",
            "Ones Tensor: \n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]]) \n",
            "\n",
            "Zeros Tensor: \n",
            " tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensor Attributes"
      ],
      "metadata": {
        "id": "kyRiryVs0_2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.rand(3, 4)\n",
        "print(f\"Shape of tensor: {tensor.shape}\")\n",
        "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
        "print(f\"Device tensor is stored on: {tensor.device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tHSkdCJ1H31",
        "outputId": "9efc2c83-b44a-4df0-bd2a-0548d828b734"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of tensor: torch.Size([3, 4])\n",
            "Datatype of tensor: torch.float32\n",
            "Device tensor is stored on: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensor Operations"
      ],
      "metadata": {
        "id": "AoppzsJc1IDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We move our tensor to the GPU if available\n",
        "if torch.cuda.is_available():\n",
        "  tensor = tensor.to('cuda')\n",
        "  print(f\"Device tensor is stored on: {tensor.device}\")"
      ],
      "metadata": {
        "id": "8Bm5kV5Y1P2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a37e8f1-e16c-4eda-fc2f-c5457ae1fbd9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device tensor is stored on: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.ones(4, 4)\n",
        "tensor[:,1] = 0\n",
        "print(tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdSx2A7o1ZVm",
        "outputId": "86246d32-4a2b-45fb-a380-0c9cd8122392"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
        "print(t1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyfHCQa81bq5",
        "outputId": "9e19ec34-d449-4ff3-cb32-d26de096e8d3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This computes the element-wise product\n",
        "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
        "# Alternative syntax:\n",
        "print(f\"tensor * tensor \\n {tensor * tensor}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sw5HBs6P1btz",
        "outputId": "313db46b-46d2-4bdf-caa7-5a73735dda31"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor.mul(tensor) \n",
            " tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]]) \n",
            "\n",
            "tensor * tensor \n",
            " tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n",
        "# Alternative syntax:\n",
        "print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fw2eWrVL1bxn",
        "outputId": "96b96b5a-bcde-42d8-843f-abc34df62eb1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor.matmul(tensor.T) \n",
            " tensor([[3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.]]) \n",
            "\n",
            "tensor @ tensor.T \n",
            " tensor([[3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor, \"\\n\")\n",
        "tensor.add_(5)\n",
        "print(tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H__5qmwH1b0j",
        "outputId": "c00a860d-6dfe-4e83-b4c6-219a06009af2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]]) \n",
            "\n",
            "tensor([[6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bridge with NumPy"
      ],
      "metadata": {
        "id": "do9JK0Ju1l6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.ones(5)\n",
        "print(f\"t: {t}\")\n",
        "n = t.numpy()\n",
        "print(f\"n: {n}\")\n",
        "\n",
        "t.add_(1)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaTZ5WgM1mqU",
        "outputId": "263e5292-d359-4371-dff6-bafbf057bfe9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t: tensor([1., 1., 1., 1., 1.])\n",
            "n: [1. 1. 1. 1. 1.]\n",
            "t: tensor([2., 2., 2., 2., 2.])\n",
            "n: [2. 2. 2. 2. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = np.ones(5)\n",
        "t = torch.from_numpy(n)\n",
        "\n",
        "np.add(n, 1, out=n)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjVZ6O8N1wEW",
        "outputId": "5d34429d-04bb-4fc9-95b4-d6c714b3fc12"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
            "n: [2. 2. 2. 2. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Differentiation in Autograd"
      ],
      "metadata": {
        "id": "iFiTLeX412o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "a = torch.tensor([2., 3.], requires_grad=True)\n",
        "b = torch.tensor([6., 4.], requires_grad=True)"
      ],
      "metadata": {
        "id": "aOkFToY713aa"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q = 3*a**3 - b**2"
      ],
      "metadata": {
        "id": "9-PH877j15xb"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "external_grad = torch.tensor([1., 1.])\n",
        "Q.backward(gradient=external_grad)"
      ],
      "metadata": {
        "id": "tfuwR_jl19tt"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if collected gradients are correct\n",
        "print(9*a**2 == a.grad)\n",
        "print(-2*b == b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jbd1UFcX2Cij",
        "outputId": "a3f91f9a-6a69-449a-f044-444801b2d398"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([True, True])\n",
            "tensor([True, True])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1"
      ],
      "metadata": {
        "id": "O_OE5wwi2KM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# point x1, x2 (1, 1)\n",
        "x1 = torch.tensor(1., requires_grad=True)\n",
        "x2 = torch.tensor(1., requires_grad=True)"
      ],
      "metadata": {
        "id": "Oq4CILZe5ImA"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q = (3*x1 - 2*x2 - 2) ** 2"
      ],
      "metadata": {
        "id": "3w75QgFI5JKN"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "external_grad = torch.tensor(1.)\n",
        "Q.backward(gradient=external_grad)"
      ],
      "metadata": {
        "id": "S77K43ya5LJb"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if collected gradients are correct\n",
        "# Manually differenatiated\n",
        "dx1 = 2 * (3*x1 - 2*x2 - 2) * (3)\n",
        "dx2 = 2 * (3*x1 - 2*x2 - 2) * (-2)\n",
        "\n",
        "print(dx1 == x1.grad)\n",
        "print(dx2 == x2.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGyn1_o75LM5",
        "outputId": "83909802-5e6b-46d2-dee0-5571d0887b1e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(True)\n",
            "tensor(True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the network"
      ],
      "metadata": {
        "id": "iI89zBKv62ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    # an affine operation: y = Wx + b\n",
        "    # 784 is the input dimension, and 68 is the output dimenstion of the first hidden layer\n",
        "    self.fc1 = nn.Linear(784, 64)\n",
        "    self.fc2 = nn.Linear(64, 64)\n",
        "    self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # apply the first layer with relu activation\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFMT_Uhb65dv",
        "outputId": "12c5e714-137e-41a6-e1be-3abc46013948"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
            "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = list(net.parameters())\n",
        "print(len(params))\n",
        "\n",
        "for p in params:\n",
        "  print(p.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-3_5IYo7ZHX",
        "outputId": "3150787d-b3db-4c39-8019-cb9b3613459b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "torch.Size([64, 784])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 64])\n",
            "torch.Size([64])\n",
            "torch.Size([10, 64])\n",
            "torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2: Identify what are the parameters that are printed in the previous code"
      ],
      "metadata": {
        "id": "BaigovjO7dH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.Size([64, 784]): This represents the weight matrix for the first layer of the neural network. It has a size of 64 (output features) by 784 (input features).\n",
        "\n",
        "torch.Size([64]): This represents the bias vector for the first layer. It has a size of 64, corresponding to the number of output features in the first layer.\n",
        "\n",
        "torch.Size([64, 64]): This represents the weight matrix for the second layer of the neural network. It has a size of 64 (output features) by 64 (input features).\n",
        "\n",
        "torch.Size([64]): This represents the bias vector for the second layer. It has a size of 64, corresponding to the number of output features in the second layer.\n",
        "\n",
        "torch.Size([10, 64]): This represents the weight matrix for the third (output) layer of the neural network. It has a size of 10 (output classes) by 64 (input features from the previous layer).\n",
        "\n",
        "torch.Size([10]): This represents the bias vector for the third layer. It has a size of 10, corresponding to the number of output classes in the network."
      ],
      "metadata": {
        "id": "Z3-di0fk80Oe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.randn(1, 784)\n",
        "out = net(input)\n",
        "\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mM8hrnFM7pPM",
        "outputId": "aa4f86fc-e74e-454b-c597-1e4c515b49d6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.1554, -0.0481,  0.1296, -0.0187,  0.0441,  0.0980, -0.0174, -0.0033,\n",
            "          0.0872,  0.0829]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3: Try the previous network with a random mini-batch of size 4 and print its output."
      ],
      "metadata": {
        "id": "fJcyDW897pEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.randn(4, 784)\n",
        "out = net(input)\n",
        "\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9kQ4Rsa7083",
        "outputId": "81894834-a969-4a70-ce3f-b58e0a615923"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0164,  0.1036,  0.0232, -0.1311, -0.0836,  0.1225, -0.0797, -0.0839,\n",
            "          0.1024,  0.0334],\n",
            "        [-0.2248,  0.0773, -0.0394, -0.0720,  0.2140,  0.1363,  0.0359,  0.0240,\n",
            "          0.1486,  0.1178],\n",
            "        [-0.2064,  0.0671,  0.0984,  0.0570,  0.0925, -0.0027,  0.0345, -0.0025,\n",
            "         -0.0196, -0.0585],\n",
            "        [-0.1450,  0.0151,  0.0862, -0.0228, -0.0334,  0.0835,  0.0803, -0.0439,\n",
            "         -0.0566, -0.0404]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define a Loss function and optimizer"
      ],
      "metadata": {
        "id": "_EMckvxp78gH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "hw6ndHlm79EV"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading a Dataset"
      ],
      "metadata": {
        "id": "vD6lgATDKzef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "training_data = datasets.MNIST(\n",
        "  root=\"data\",\n",
        "  train=True,\n",
        "  download=True,\n",
        "  transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "  root=\"data\",\n",
        "  train=False,\n",
        "  download=True,\n",
        "  transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "SBSMtqoC9I9j"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iterating and Visualizing the Dataset"
      ],
      "metadata": {
        "id": "4gEcnSesKrGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3, 3\n",
        "\n",
        "for i in range(1, cols * rows + 1):\n",
        "  sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
        "  img, label = training_data[sample_idx]\n",
        "  figure.add_subplot(rows, cols, i)\n",
        "  plt.title(\"digit:\" + str(label))\n",
        "  plt.axis(\"off\")\n",
        "  plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "_zhhbznC9P5i",
        "outputId": "b75901e5-6f9f-4b6e-b616-f24a0df86bef"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 9 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAn4AAAKSCAYAAABMVtaZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA9dklEQVR4nO3de3zP9f//8cfbzpshrKFhzPmUiMwnNoWVLxKSQqFIRak+dNyByFfHT6S2kMqhnD6R1FBRlFIhlBwScphmzjY0e/3+6Ne+5Pl822t773163q6XS394PPd4vZ5bnnbfa+/n8+2wLMsSAAAA+L0ynp4AAAAA3IPgBwAAYAiCHwAAgCEIfgAAAIYg+AEAABiC4AcAAGAIgh8AAIAhCH4AAACGIPgBAAAYguDnImlpaeJwOAr/HBsbKwMHDizWtRITEyUxMdE1EwP8DGsNcA/Wmn8i+PmAAwcOSFpammzcuLHIPX/++aeMGTNGateuLSEhIVK7dm0ZN26c5Ofnl95EAR9XnLW2fPlyueeee6RJkyYSEBAgsbGxpTY/wF/YXWu7d+8Wh8Oh/W/IkCGlO2E/EujpCfirbdu2SZkyxcvVy5cvv+jPBw4ckDFjxkhsbKw0b968SNfo37+/zJ8/XwYPHizXXnutfPPNN5KcnCx79+6VN998s1jzAryRp9fanDlzZO7cudKiRQupVq1aseYB+AJPrrWoqCiZOXPmJfXMzEyZPXu2dO7cuVjzMhHBr5SEhIQUuzc4OLhE9/7uu+9k3rx5kpycLGPHjhURkWHDhknlypXl5ZdfluHDh0uzZs1KdA/AW3hyrYmIPPfcczJ16lQJCgqSrl27ypYtW0p8TcAbeXKtRURESP/+/S+pv/3221KuXDnp1q1bia5vEn7VWwxr1qyRVq1aSWhoqMTFxUlGRsYlH6N6LcSmTZskISFBwsLCJCYmRsaNGyczZswQh8Mhu3fvLvy4C18LsWrVKmnVqpWIiAwaNKjwsfbbb78tIiK5ubnyyy+/yOHDhwv7V69eLSIiffv2vej+ffv2FcuyZO7cuSX8CgDu4e1rTUSkWrVqEhQU5LLPGfAEX1hr/3Tw4EFZuXKl9OzZU0JDQ4v9uZuGJ342bd68WTp37ixRUVGSlpYm+fn5kpqaKtHR0U779u/fLx06dBCHwyFPPvmkREREyLRp0y77E1TDhg1l7NixkpKSIkOHDpV27dqJiEjbtm1FRGTdunXSoUMHSU1NlbS0NBEROXv2rIiIhIWFXXSt8PBwERH54YcfbH/egLv5wloD/IGvrrX3339fCgoKpF+/fvY+YcMR/GxKSUkRy7Jk9erVUqNGDRER6dWrlzRt2tRp38SJE+Xo0aOyfv36wtczDBo0SOrWreu0Lzo6Wm6++WZJSUmR+Ph45aPuf6pfv76IiHz11VdSq1atwvrfTwL3799/2WsAnuYLaw3wB7661mbPni1Vq1aVG264oVj9puJXvTacP39eli1bJj169ChcHCJ//fSSlJTktDczM1Pi4+MvehFrxYoVS/yTSmJioliWddFPRV26dJGaNWvKv//9b/nvf/8re/bskXnz5snTTz8tgYGBkpeXV6J7AqXNV9Ya4Ot8da1t375dfvjhB+nbt2+xN5yYiq+WDdnZ2ZKXl6f8aebvp2w6e/bskTp16lxSV9VKKjQ0VJYuXSqVKlWSXr16SWxsrNx1112SkpIiFStWlLJly7r8noAr+cpaA3ydr6612bNni4jwa95i4Fe9fqpx48ayZcsW+fnnn+Xo0aPSqFEjCQsLk0ceeUQSEhI8PT0AAIptzpw5Ur9+fWnZsqWnp+JzCH42REVFSVhYmOzYseOSsW3btjntrVmzpuzcufOSuqr2TxeenG6Hw+GQxo0bF/75448/loKCAunYsWOxrge4i6+tNcBX+eJa+/bbb2Xnzp2Fx5XBHn7Va0NAQIAkJSXJokWLZO/evYX1rVu3yrJly5z2JiUlydq1ay86pfzIkSOFj6udiYiIEBGRY8eOXTJW1G3veXl5kpycLFWrVpU77rjjsvcEPMmX1xrgS3xxrc2ZM0dERO68887L3geX4omfTWPGjJHMzExp166dPPDAA5Kfny+TJ0+Wxo0by6ZNm7R9o0ePllmzZkmnTp1kxIgRhdvea9SoIUeOHHH6009cXJxUqFBB0tPTJTIyUiIiIuS6666TWrVqabe99+nTR6pVqyaNGjWSEydOyFtvvSW7du2SpUuXSmRkpCu/JECp8JW1tmnTJvnwww9F5K8nHcePH5dx48aJiMjVV1/NwbLwer6y1kT+2owyd+5cadOmjcTFxbnqS2AWC7Z98cUXVsuWLa3g4GCrdu3aVnp6upWammpd+OWsWbOmdffdd1/Ut2HDBqtdu3ZWSEiIFRMTY02YMMGaNGmSJSJWVlZW4cclJCRYCQkJF/UuXrzYatSokRUYGGiJiDVjxgzLsixr5cqVlohYqampF338xIkTrQYNGlihoaHWFVdcYXXv3t3asGGDC78KQOnzhbU2Y8YMS0SU//1zXoC38oW1ZlmWlZmZaYmINWnSJFd96sZxWJZluTlr4gIjR46UjIwMOXXqlAQEBHh6OoDfYq0B7sFa8268xs+N/nl+Xk5OjsycOVOuv/56FgfgQqw1wD1Ya76H1/i5UXx8vCQmJkrDhg3l0KFDMn36dDlx4oQkJyd7emqAX2GtAe7BWvM9BD836tKliyxYsEDefPNNcTgc0qJFC5k+fbq0b9/e01MD/AprDXAP1prv4TV+AAAAhuA1fgAAAIYg+AEAABiC4AcAAGCIIm/u4D0s4Y+88SWurDX4I9Ya4B6XW2s88QMAADAEwQ8AAMAQBD8AAABDEPwAAAAMQfADAAAwBMEPAADAEAQ/AAAAQxD8AAAADEHwAwAAMATBDwAAwBAEPwAAAEMQ/AAAAAxB8AMAADAEwQ8AAMAQBD8AAABDEPwAAAAMQfADAAAwBMEPAADAEAQ/AAAAQxD8AAAADEHwAwAAMATBDwAAwBAEPwAAAEMQ/AAAAAxB8AMAADAEwQ8AAMAQBD8AAABDEPwAAAAMQfADAAAwBMEPAADAEIGengAA+JIhQ4Yo6xkZGdqeLl26KOuZmZkumRMAFBVP/AAAAAxB8AMAADAEwQ8AAMAQBD8AAABDEPwAAAAMQfADAAAwBMe5AMA/VKtWTTv20EMPKeuWZZXWdADAZXjiBwAAYAiCHwAAgCEIfgAAAIYg+AEAABiC4AcAAGAIdvV6SLly5bRjtWvXVtbvuecebc/VV1+trH/99dfanvHjxyvrJ0+e1PYAJnj88ce1Y40aNXLjTICiq1+/vrLepk0bbc+2bduU9QoVKmh7evfubWteIiK33nqrsp6fn6/tWbJkibK+f/9+bc+0adOU9d9//93J7MzCEz8AAABDEPwAAAAMQfADAAAwBMEPAADAEAQ/AAAAQxD8AAAADOGwivjO4g6Ho7Tn4rOCgoK0Y3fffbey/vDDD2t7mjRpUuI5FcWPP/6orN9yyy3anj179pTWdDyiiH/93Yq15j4BAQHK+rvvvqvt6du3r7L+zTffaHt69eqlrGdlZTmZnX9hrbnGsGHDtGPPP/+8sh4ZGVla0/E62dnZyrqzNZ2cnKys5+XluWRO7na5tcYTPwAAAEMQ/AAAAAxB8AMAADAEwQ8AAMAQBD8AAABDsKvXBt3u3enTp2t7BgwYoKw7+7KvWbNGWX/22We1Pddcc42yPnHiRG2PjrNdxT/99JPt63kzdhqa7d5771XWMzIytD25ubnKerdu3bQ9q1atsjUvf8Rac41ff/1VO1a7dm03zsR/REVFKeuHDx9280xcg129AAAAEBGCHwAAgDEIfgAAAIYg+AEAABiC4AcAAGAIgh8AAIAhAj09AW/TvHlz7VhmZqayHh0dre3ZvXu3sj548GBtz8qVK7VjOnfddZftHp1KlSq57FqAN9O9Obsz+/fvV9Y5sgX+5Pz589qxo0ePuuw+WVlZ2rGPPvpIWY+JidH29OzZU1kPDw/X9tx+++3K+pQpU7Q9vownfgAAAIYg+AEAABiC4AcAAGAIgh8AAIAhCH4AAACGMHZXb2Cg+lN/+eWXtT263bunTp3S9vTu3VtZ/+GHH5zMTq1Hjx7FGrPrzjvv1I59+eWXLrsP4GnVq1dX1i/3JueAp4wcOVI7NnToUGV9zpw52p68vDxl/eTJk9qezz77TDvmabqduGvXrtX26Hbq+yue+AEAABiC4AcAAGAIgh8AAIAhCH4AAACGIPgBAAAYguAHAABgCL8+zqVJkybasTFjxijrHTp00PZs3rxZWXd2lMquXbu0Yzpdu3ZV1t977z3b1wJMd/XVV7v0ehkZGS69HmDHkiVLijVmisOHD9vuadCgQSnMxHvxxA8AAMAQBD8AAABDEPwAAAAMQfADAAAwBMEPAADAEH6xqzcmJkZZX7FihbbnyiuvVNZffPFFbY9uLCcnR9vTokULZb1Pnz7ankcffVRZ/+6777Q93bt3V9Z///13bU9YWJiynp2dre0BfE1KSortnj/++EM7lp6eXpLpAChFd955p6en4PV44gcAAGAIgh8AAIAhCH4AAACGIPgBAAAYguAHAABgCIIfAACAIfziOJdWrVop61WqVNH2bNy4UVkfNWqUtqdjx47Kerdu3bQ9Dz30kHZMZ926dcr6Lbfcou1xdqSMXe+//77LrgW4y/3336+sO1s3Os8//7x2LC8vz/b1ALhO2bJltWP33HOP7et9++23JZmOz+GJHwAAgCEIfgAAAIYg+AEAABiC4AcAAGAIgh8AAIAhfGZXb3BwsHZszpw5tq9Xv359ZT07O1vbU6FCBWU9MFD/ZczNzVXWZ82ape154oknlPWjR49qe4pj9+7dyrqzrwHgrYKCgpT1MmX0P986HA5lnZ27gPf617/+pR2rUaOG7ett3ry5JNPxOTzxAwAAMATBDwAAwBAEPwAAAEMQ/AAAAAxB8AMAADAEwQ8AAMAQPnOcS35+vnZs7dq1ynqHDh20PWFhYbbqIiJbt25V1j/77DNtz0svvaSs645SKa6EhARl3dkxOL/99puy/scff7hkToA7dezYUVm3LEvbk5WVpaynp6e7ZE4Aiq969erK+muvvabt0WWF4cOHa3tycnLsTczH8cQPAADAEAQ/AAAAQxD8AAAADEHwAwAAMATBDwAAwBA+s6u3oKBAOzZq1ChlfeTIkdqet956S1k/ffq0tke3q/fkyZPaHnepU6eOsh4QEKDt2bZtW2lNBygV3bt3144528WvM3bs2JJMB0AJlSmjf/40d+5cZV33/U5E5ODBg8p6RkaGvYn5MZ74AQAAGILgBwAAYAiCHwAAgCEIfgAAAIYg+AEAABiC4AcAAGAInznOxZkffvhBWR8wYICbZ1K6goODtWMPPPCA7estX768JNMB3O7FF1/UjoWHh9u+Xnp6ekmmA6CEnnrqKe1YfHy87evpjoDB/+GJHwAAgCEIfgAAAIYg+AEAABiC4AcAAGAIgh8AAIAh/GJXrylat26tHWvRooUbZwL4Dnb5AZ4XFBSkrI8aNcr2tVauXKkdGz16tO3rmYYnfgAAAIYg+AEAABiC4AcAAGAIgh8AAIAhCH4AAACGIPgBAAAYguNcAHidqlWrKuthYWG2rzV27NiSTgdAEZQrV0479sILL9juyc/PV9bT0tK0PX/++ad2DH/hiR8AAIAhCH4AAACGIPgBAAAYguAHAABgCIIfAACAIdjV6+cOHDigHVuyZIkbZwIU3fXXX6+sV6tWTduzY8cOZX337t2umBKAy/jggw+0YzfccIOy7mwX7n/+8x9l/csvv7Q1L1yMJ34AAACGIPgBAAAYguAHAABgCIIfAACAIQh+AAAAhiD4AQAAGILjXPxcQUGBdkz3BtiAO8TExGjHnnrqKdvXy8zMVNbPnDlj+1oA9O69915lvUOHDravNXfuXO3Y6NGjbV8Pl8cTPwAAAEMQ/AAAAAxB8AMAADAEwQ8AAMAQBD8AAABDsKsXgEc0bdpUO9asWTPb1/v+++9LMh0AF2jZsqV27I033lDWHQ6HtufYsWPK+sSJE23NCyXHEz8AAABDEPwAAAAMQfADAAAwBMEPAADAEAQ/AAAAQxD8AAAADMFxLn7u9ddf9/QUAKVff/1VO3bo0CFlPTo6Wtvz6aeflnhOAP4ye/Zs7VhgoP3o0L9/f2V9y5Yttq+FkuGJHwAAgCEIfgAAAIYg+AEAABiC4AcAAGAIgh8AAIAh2NXr56pXr+7pKQBK27dv145Vq1bNjTMBzFW7dm1lvWbNmravlZOTox3bvHmz7euhdPDEDwAAwBAEPwAAAEMQ/AAAAAxB8AMAADAEwQ8AAMAQBD8AAABDcJyLDzl48KB27PDhw8p6bGxsKc0GAODrdu3apazPnz9f2zNgwABlffHixS6ZE0oXT/wAAAAMQfADAAAwBMEPAADAEAQ/AAAAQxD8AAAADOGwLMsq0gc6HKU9F8DtivjX361Ya/BHrDXAPS631njiBwAAYAiCHwAAgCEIfgAAAIYg+AEAABiC4AcAAGAIgh8AAIAhinycCwAAAHwbT/wAAAAMQfADAAAwBMEPAADAEAQ/AAAAQxD8AAAADEHwAwAAMATBDwAAwBAEPwAAAEMQ/AAAAAxB8AMAADAEwQ8AAMAQBD8AAABDEPwAAAAMQfBzkbS0NHE4HIV/jo2NlYEDBxbrWomJiZKYmOiaiQF+hrUGuAdrzT8R/HzAgQMHJC0tTTZu3FjknuXLl8s999wjTZo0kYCAAImNjS21+QH+ojhrLTExURwOxyX/3XTTTaU3UcDHFWetiYh8/fXXcv3110t4eLhUqVJFHnroITl16lTpTNJPBXp6Av5q27ZtUqZM8XL18uXLL/rzgQMHZMyYMRIbGyvNmzcv0jXmzJkjc+fOlRYtWki1atWKNQ/AF3h6rYmIxMTEyIQJEy6qse7gbzy91jZu3Cg33nijNGzYUF5++WXZt2+fvPjii7Jjxw755JNPijUvExH8SklISEixe4ODg0t8/+eee06mTp0qQUFB0rVrV9myZUuJrwl4I0+vNRGR8uXLS//+/V1yLcBbeXqtPfXUU3LFFVfIqlWrpFy5ciLy16+fhwwZIsuXL5fOnTuX+B4m4Fe9xbBmzRpp1aqVhIaGSlxcnGRkZFzyMarXQmzatEkSEhIkLCxMYmJiZNy4cTJjxgxxOByye/fuwo+78LUQq1atklatWomIyKBBgwp/jfT222+LiEhubq788ssvcvjw4YvuVa1aNQkKCnLZ5wx4gi+stb/l5+fzKyf4LG9faydOnJAVK1ZI//79C0OfiMhdd90lZcuWlXnz5rnmC2EAnvjZtHnzZuncubNERUVJWlqa5OfnS2pqqkRHRzvt279/v3To0EEcDoc8+eSTEhERIdOmTbvsT1ANGzaUsWPHSkpKigwdOlTatWsnIiJt27YVEZF169ZJhw4dJDU1VdLS0lzyOQLewJfW2vbt2yUiIkLOnTsn0dHRMmTIEElJSeGHL/gEX1hrmzdvlvz8fLn22msvulZwcLA0b95cNmzYUMzP3jwEP5tSUlLEsixZvXq11KhRQ0REevXqJU2bNnXaN3HiRDl69KisX7++8PUMgwYNkrp16zrti46OlptvvllSUlIkPj6eXyfBGL6y1uLi4qRDhw7StGlTOX36tCxYsEDGjRsn27dvl7lz5xbpGoAn+cJaO3jwoIiIVK1a9ZKxqlWryurVqy97DfyFX/XacP78eVm2bJn06NGjcHGI/PXTS1JSktPezMxMiY+Pv+hFrBUrVpR+/fqVaE6JiYliWRZP++BXfGmtTZ8+XVJTU6Vnz54yYMAAWbx4sQwZMkTmzZsn33zzTYnuCZQ2X1lreXl5IqJ+nWFoaGjhOC6P4GdDdna25OXlKX+aqV+/vtPePXv2SJ06dS6pq2qA6Xx9rT322GMiIvLpp5+67Z5AcfjKWgsLCxMRkbNnz14ydubMmcJxXB7BDwBcrHr16iIicuTIEQ/PBPAPf/+K9+9f+V7o4MGDHJ9kA8HPhqioKAkLC5MdO3ZcMrZt2zanvTVr1pSdO3deUlfV/unCk9MBE/j6Wtu1a5eI/PV5AN7MV9ZakyZNJDAwUL7//vuL6ufOnZONGzfaOnfTdAQ/GwICAiQpKUkWLVoke/fuLaxv3bpVli1b5rQ3KSlJ1q5de9Ep5UeOHJHZs2df9r4REREiInLs2LFLxi53xATgi3xlrZ04ceKSXz1ZliXjxo0rnAvgzXxlrZUvX146duwos2bNkpMnTxbWZ86cKadOnZLbbrvtsvfEX9jVa9OYMWMkMzNT2rVrJw888IDk5+fL5MmTpXHjxrJp0yZt3+jRo2XWrFnSqVMnGTFiROG29xo1asiRI0ec/vQTFxcnFSpUkPT0dImMjJSIiAi57rrrpFatWtojJjZt2iQffvihiPz109fx48cLvxldffXV0q1bN9d8QYBS4gtrbf369XLHHXfIHXfcIXXq1JG8vDz54IMP5KuvvpKhQ4dKixYtXP1lAVzOF9aaiMj48eOlbdu2kpCQIEOHDpV9+/bJSy+9JJ07d+YtEu2wYNsXX3xhtWzZ0goODrZq165tpaenW6mpqdaFX86aNWtad99990V9GzZssNq1a2eFhIRYMTEx1oQJE6xJkyZZImJlZWUVflxCQoKVkJBwUe/ixYutRo0aWYGBgZaIWDNmzLAsy7JWrlxpiYiVmpp60cfPmDHDEhHlf/+cF+CtvH2t7dq1y7rtttus2NhYKzQ01AoPD7datmxppaenWwUFBa7+cgClxtvX2t9Wr15ttW3b1goNDbWioqKsBx980Dpx4oSrvgxGcFiWZbk7bOL/jBw5UjIyMuTUqVMSEBDg6ekAfou1BrgHa8278Ro/N/rnOUM5OTkyc+ZMuf7661kcgAux1gD3YK35Hl7j50bx8fGSmJgoDRs2lEOHDsn06dPlxIkTkpyc7OmpAX6FtQa4B2vN9xD83KhLly6yYMECefPNN8XhcEiLFi1k+vTp0r59e09PDfArrDXAPVhrvofX+AEAABiC1/gBAAAYguAHAABgCIIfAACAIYq8uYP3i4U/8saXuLLW4I9Ya4B7XG6t8cQPAADAEAQ/AAAAQxD8AAAADEHwAwAAMATBDwAAwBAEPwAAAEMQ/AAAAAxB8AMAADAEwQ8AAMAQBD8AAABDEPwAAAAMQfADAAAwBMEPAADAEAQ/AAAAQxD8AAAADEHwAwAAMATBDwAAwBAEPwAAAEMQ/AAAAAxB8AMAADAEwQ8AAMAQBD8AAABDEPwAAAAMQfADAAAwBMEPAADAEIGengBcY9GiRbZ7evTo4fJ5AAAA78UTPwAAAEMQ/AAAAAxB8AMAADAEwQ8AAMAQBD8AAABDsKvXh7Rp00Y7duONNyrrR44cKa3pAKWmQoUKynqfPn20PXfeeaeyvnHjRhfM6PKaN2+uHdPN4fbbb9f2REdHK+tbtmzR9jRr1kw7BgAiPPEDAAAwBsEPAADAEAQ/AAAAQxD8AAAADEHwAwAAMATBDwAAwBAc5+JDqlevrh0LCwtz40yA0vXJJ58o661bt7Z9rXbt2mnHHA6Hsm5Zlu37FHcOOro5LF68uKTTAWAwnvgBAAAYguAHAABgCIIfAACAIQh+AAAAhiD4AQAAGIJdvT5k+/bt2rFTp065cSZAycXFxWnHGjVqZPt6e/fuVdZXrFhh+1rFkZubqx3bt2+fsn769GltT35+vrK+YMECexMDfFRUVJSyPnz4cG1P5cqVlfXJkydrew4ePKisHz9+3MnsfBdP/AAAAAxB8AMAADAEwQ8AAMAQBD8AAABDEPwAAAAMQfADAAAwBMe5+JA33nhDOxYZGamsL1mypLSmAxRJUFCQsj5r1ixtT9myZZX18+fPa3smTZqkrL/yyitOZgfAVaKjo7VjuiOa+vXrp+3p0aOHsl6xYkVb8xIRuf/++7VjW7duVdZfffVVbc+0adOU9YKCAnsT8wCe+AEAABiC4AcAAGAIgh8AAIAhCH4AAACGIPgBAAAYgl29PsSyLNtjS5cuLa3pAEXyP//zP8p669atbV/r5Zdf1o6xexdwndDQUO1YfHy8sj579mxtT5UqVZT106dPa3sOHDigrOfk5Gh7dGrVqqUd0+04Tk9P1/aEhIQo66+99pq2x9n3cHfiiR8AAIAhCH4AAACGIPgBAAAYguAHAABgCIIfAACAIQh+AAAAhnBYRdxf7HA4Snsu+P8qVaqkrK9Zs0bbU7duXWU9MJATe5zxlu31F/LFtdamTRvt2JIlS5R1Z2+0fubMGWXd2ZEMf/zxh3YMnsda807Vq1dX1nVHtoiIvP/++7bv89tvvynrTz/9tEvvo9OtWzft2C233KKsDx482PZ9pkyZoh0bMWKE7esVx+XWGk/8AAAADEHwAwAAMATBDwAAwBAEPwAAAEMQ/AAAAAzBlk8vpNuhq6uLiEydOrW0pgNc1tixY7Vjznbv6ujeAP2VV17R9mzZssX2fcqUUf/sW1BQoO3RvRG9szeOd/ZG9EBpa9mypXZs0aJFyvpVV11l+z5jxozRjul2ux4+fNj2fYpDd7qAiMhPP/2krGdnZ2t7Ro0apazr/u3yJjzxAwAAMATBDwAAwBAEPwAAAEMQ/AAAAAxB8AMAADAEwQ8AAMAQHOfihRITE5V1Z28oznERcIcmTZoo6+3bt3fpfXR/1/v27euW+zh7k/Nx48Yp62fOnNH29O7dW1n/5JNPnMwOuFRwcLB27IYbblDWnR33pTu2xdn3lKefflpZnzNnjrbHXce2FMeuXbuU9W+//Vbbo/s3omzZsi6ZU2niiR8AAIAhCH4AAACGIPgBAAAYguAHAABgCIIfAACAIdjV6yGVKlXSjt1///3KurOdhs7GAFfZsmWLsr59+3ZtT+PGjUtrOl4lNDRUO6bb7RgfH6/t+eWXX0o8J/iuoKAgZf1///d/tT0jR460fZ+tW7cq6xMnTtT2vPvuu7bv44tq1aqlHQsICFDWd+7cWVrTcRme+AEAABiC4AcAAGAIgh8AAIAhCH4AAACGIPgBAAAYguAHAABgCI5z8ZDw8HDtmO5Ns52ZO3duSaYDlEhCQoJ2rHz58rav17p1a2Xd2fEnxfHwww+79Ho65cqVU9anTJmi7enWrZuynpub65I5wbsNHz5cWS/OkS379u3TjiUmJirr2dnZtu/jb+666y5PT6FU8MQPAADAEAQ/AAAAQxD8AAAADEHwAwAAMATBDwAAwBAOy7KsIn2gw1HaczHKLbfcoh1buHCh7evFxsYq6852c0GkiH/93Yq15nk1atTQjunevL5v377anuL8PatXr56y7gtvAq/CWruUbueuiMjTTz+trEdHR2t7srKylPWbb75Z2/Pjjz9qx/yJszX91ltvKevOTisICAhQ1mvXrq3t2b17t3bMlS631njiBwAAYAiCHwAAgCEIfgAAAIYg+AEAABiC4AcAAGAIgh8AAIAhAj09AVMlJydrx3RHDMyYMUPbw7EtgOvs3btXO/bZZ58p63369HHpHO6++25l3dm/HfBOTZo0UdafeeYZbc+VV16prK9Zs0bbc//99yvrP/30k5PZ+Z6YmBjt2L333qusDxo0SNtTvXp1Zf3XX3/V9mRkZCjre/bs0fZ4C574AQAAGILgBwAAYAiCHwAAgCEIfgAAAIYg+AEAABiCXb2lrFq1asr6Nddco+3RvcHytGnTXDInAID7vPbaa8q6bueuM9HR0dqxpk2b2qq72pYtW7Rjup3Nzuh26NavX1/bc9VVV9m+j07Xrl21Y9u2bXPZfdyNJ34AAACGIPgBAAAYguAHAABgCIIfAACAIQh+AAAAhiD4AQAAGILjXErZ7bff7ukpAAA86LnnnlPW27dvb/tadevW1Y7NmTPH9vVc6ejRo9qxK664wo0zseeJJ55Q1nfu3OnmmbgHT/wAAAAMQfADAAAwBMEPAADAEAQ/AAAAQxD8AAAADMGuXi908OBBW3UAviknJ0c79tFHH7lxJihN69atU9br1aun7Rk9erTt++h2CTu7jyu5eufunj17lPXevXtre7Kzs23fZ//+/cr6+fPnbV/LF/DEDwAAwBAEPwAAAEMQ/AAAAAxB8AMAADAEwQ8AAMAQBD8AAABDcJxLKYuMjFTWy5TRZ+6ZM2cq67qt7YCnvfLKK9qxsmXLKutDhgwpremUWJUqVbRjQ4cOddl9pkyZoh379ttvXXYfeNaxY8ds1UWK9/esfPnyynpYWJi2p1evXsp6gwYNbN/fGd33tb1792p7zp49q6wfPXrUJXMyFU/8AAAADEHwAwAAMATBDwAAwBAEPwAAAEMQ/AAAAAzBrl4XaNOmjXZs1KhRynpBQYG2Z/369SWeE1AadDsN77//fm3Pxo0blfWAgABtj7veHF23u37y5MnanpYtW9q+z4EDB5T1t956y/a1AJ3jx4/bqos431kO/8QTPwAAAEMQ/AAAAAxB8AMAADAEwQ8AAMAQBD8AAABDEPwAAAAMwXEuLlC9enXtmO7Nsf/44w9tz8qVK0s8J6C4ypYtqx3797//rawHBQVpe1q1aqWsL1y4UNujO/7k9ddf1/YUxzPPPKOs9+zZ0/a1nB1B8+qrryrrv//+u+37AEBJ8MQPAADAEAQ/AAAAQxD8AAAADEHwAwAAMATBDwAAwBDs6vWQc+fOacdycnLcOBPgYnFxcdqxK6+80mX36datm+2e++67z2X3FxFxOBzKumVZ2p69e/cq64MHD9b2sFMfgLfgiR8AAIAhCH4AAACGIPgBAAAYguAHAABgCIIfAACAIQh+AAAAhnBYzs4tuPADNcceAL6siH/93cqb19pVV12lrD/88MPaHt2xLfXq1XPJnEri5MmTyvqtt96q7dm8ebOyfvjwYZfMyV+x1gD3uNxa44kfAACAIQh+AAAAhiD4AQAAGILgBwAAYAiCHwAAgCHY1QujsdMQcA/WGuAe7OoFAACAiBD8AAAAjEHwAwAAMATBDwAAwBAEPwAAAEMQ/AAAAAxB8AMAADAEwQ8AAMAQBD8AAABDEPwAAAAMQfADAAAwBMEPAADAEAQ/AAAAQxD8AAAADEHwAwAAMATBDwAAwBAEPwAAAEMQ/AAAAAxB8AMAADCEw7Isy9OTAAAAQOnjiR8AAIAhCH4AAACGIPgBAAAYguAHAABgCIIfAACAIQh+AAAAhiD4AQAAGILgBwAAYAiCHwAAgCEIfgAAAIYg+AEAABiC4AcAAGAIgh8AAIAhCH4ukpaWJg6Ho/DPsbGxMnDgwGJdKzExURITE10zMcDPsNYA92Ct+SeCnw84cOCApKWlycaNG4vc89xzz0mbNm0kKipKQkNDpW7dujJy5EjJzs4uvYkCPs7uWsvNzZUpU6ZI586dpWrVqhIZGSnXXHONvPHGG3L+/PnSnSzgw4rzfS0xMVEcDscl/910002lN1E/FOjpCfirbdu2SZkyxcvVy5cvv+jPBw4ckDFjxkhsbKw0b968SNf44YcfpHnz5tK3b1+JjIyUrVu3ytSpU2Xp0qWyceNGiYiIKNbcAG/jybW2a9cuGTFihNx4443y6KOPSrly5WTZsmXywAMPyDfffCPvvPNOseYFeCNPf18TEYmJiZEJEyZcVKtWrVqx5mQqgl8pCQkJKXZvcHBwie+/cOHCS2rx8fHSu3dvWbJkifTt27fE9wC8gSfXWpUqVWTz5s3SuHHjwtp9990ngwcPlhkzZkhycrLUqVOnRPcAvIWnv6+JiJQvX1769+/vkmuZil/1FsOaNWukVatWEhoaKnFxcZKRkXHJx6heC7Fp0yZJSEiQsLAwiYmJkXHjxsmMGTPE4XDI7t27Cz/uwtdCrFq1Slq1aiUiIoMGDSp8tP3222+LyF+/avrll1/k8OHDl513bGysiIgcO3bM7qcMeIS3r7XKlStfFPr+duutt4qIyNatW0vw2QPu4+1r7UL5+fly6tSpEn/OpuKJn02bN2+Wzp07S1RUlKSlpUl+fr6kpqZKdHS00779+/dLhw4dxOFwyJNPPikREREybdq0y/4E1bBhQxk7dqykpKTI0KFDpV27diIi0rZtWxERWbdunXTo0EFSU1MlLS3tol7LsiQnJ0fy8/Nlx44d8sQTT0hAQAAvsIVP8KW19k9ZWVki8lcwBLydL6217du3S0REhJw7d06io6NlyJAhkpKSIkFBQcX/AhiG4GdTSkqKWJYlq1evlho1aoiISK9evaRp06ZO+yZOnChHjx6V9evXF76eYdCgQVK3bl2nfdHR0XLzzTdLSkqKxMfH23rEfejQIalatWrhn2NiYmTOnDnSoEGDIl8D8BRfWmsXOnfunPznP/+RWrVqFT7VALyZr6y1uLg46dChgzRt2lROnz4tCxYskHHjxsn27dtl7ty5RboGCH62nD9/XpYtWyY9evQoXBwif/30kpSUJB9//LG2NzMzU+Lj4y96EWvFihWlX79+Mnny5GLPKTExUSzLUo5VrFhRVqxYIWfOnJENGzbIf//7Xx6Pwyf42lq70PDhw+Xnn3+WpUuXSmAg/8TCu/nSWps+ffpFfx4wYIAMHTpUpk6dKo888oi0adOm2Pc0Ca/xsyE7O1vy8vKUP83Ur1/fae+ePXuUL/IuzRd+BwcHS8eOHaVr166SnJwsU6ZMkXvuuUc++uijUrsn4Aq+ttb+9sILL8jUqVPl2WeflS5dupT6/YCS8tW19rfHHntMREQ+/fRTt93T1xH8DNK2bVupWrWqzJ4929NTAfzO22+/LY8//rgMGzZMnnnmGU9PBzBC9erVRUTkyJEjHp6J7yD42RAVFSVhYWGyY8eOS8a2bdvmtLdmzZqyc+fOS+qq2j9deHJ6SZ05c0aOHz/ususBpcHX1trixYvl3nvvlZ49e8qUKVOKdQ3AE3xtrf3Trl27ROSvzwNFQ/CzISAgQJKSkmTRokWyd+/ewvrWrVtl2bJlTnuTkpJk7dq1F51SfuTIkSI9ffv7sGXVMSyqbe+nT5+W3NzcSz524cKFcvToUbn22msve0/Ak3xlrYmIfPnll9K3b19p3769zJ49u9gH3AKe4Ctr7cSJE3L27NmLPs6yLBk3blzhXFA0vPLYpjFjxkhmZqa0a9dOHnjgAcnPz5fJkydL48aNZdOmTdq+0aNHy6xZs6RTp04yYsSIwm3vNWrUkCNHjjj96ScuLk4qVKgg6enpEhkZKREREXLddddJrVq1lNved+zYIR07dpTbb79dGjRoIGXKlJHvv/9eZs2aJbGxsfLwww+7+ssCuJwvrLU9e/ZI9+7dxeFwSO/evWX+/PkXXa9Zs2bSrFkzl3w9gNLiC2tt/fr1cscdd8gdd9whderUkby8PPnggw/kq6++kqFDh0qLFi1c/WXxXxZs++KLL6yWLVtawcHBVu3ata309HQrNTXVuvDLWbNmTevuu+++qG/Dhg1Wu3btrJCQECsmJsaaMGGCNWnSJEtErKysrMKPS0hIsBISEi7qXbx4sdWoUSMrMDDQEhFrxowZlmVZ1sqVKy0RsVJTUws/Njs72xo6dKjVoEEDKyIiwgoODrbq1q1rjRw50srOznb1lwMoNd6+1v6u6f678GMBb+bta23Xrl3WbbfdZsXGxlqhoaFWeHi41bJlSys9Pd0qKChw9ZfDrzksqwjnE6DUjBw5UjIyMuTUqVMSEBDg6ekAfou1BrgHa8278WIUN8rLy7vozzk5OTJz5ky5/vrrWRyAC7HWAPdgrfkeXuPnRvHx8ZKYmCgNGzaUQ4cOyfTp0+XEiROSnJzs6akBfoW1BrgHa833EPzcqEuXLrJgwQJ58803xeFwSIsWLWT69OnSvn17T08N8CusNcA9WGu+h9f4AQAAGILX+AEAABiC4AcAAGAIgh8AAIAhiry5w5XvFwt4C298iStrDf6ItQa4x+XWGk/8AAAADEHwAwAAMATBDwAAwBAEPwAAAEMQ/AAAAAxB8AMAADAEwQ8AAMAQBD8AAABDEPwAAAAMQfADAAAwBMEPAADAEAQ/AAAAQxD8AAAADEHwAwAAMATBDwAAwBAEPwAAAEMQ/AAAAAxB8AMAADAEwQ8AAMAQBD8AAABDEPwAAAAMQfADAAAwBMEPAADAEAQ/AAAAQwR6egIAUJrKlNH/fFuvXj1lfcWKFdqemJiYEs/pbwMHDtSOvfPOOy67DwD8jSd+AAAAhiD4AQAAGILgBwAAYAiCHwAAgCEIfgAAAIZgV6+HXHfdddqx7t27K+vHjx/X9jz55JPKevny5bU9DodDWbcsS9szbNgwZf3NN9/U9gDuEBAQoKynpaVpe5566inb9ykoKLDdo5OYmKgdmzlzZqnfH2aoU6eOduyZZ55R1tu0aaPtOXv2rLK+YcMGbY/u+42z72uLFy9W1g8fPqzt+fHHH7Vj+AtP/AAAAAxB8AMAADAEwQ8AAMAQBD8AAABDEPwAAAAMQfADAAAwhMNydnbHhR+o2Yrtb0aOHKkd0x29sH79em1Pr169lPUmTZrYmZbXOHfunLLerl07bc/3339fWtMpsSL+9XcrU9ZacdSrV087NmbMGGW9T58+tu/z559/ase+++47Zb1q1aranlq1atmew1VXXaWsZ2Vl2b6WN2Ctec67776rHevfv78bZ+IaeXl52rGlS5cq68X5d8BXXW6t8cQPAADAEAQ/AAAAQxD8AAAADEHwAwAAMATBDwAAwBCBnp6ApzRv3lxZHz9+vLYnNDRUWe/WrZsrplQqjh49qh0LDw9X1kNCQrQ9wcHBynrZsmXtTQwohqefflo7ptu1p9uJLqLfcf7cc89pez755BNl/ZZbbtH2zJ8/X1kPCAjQ9rCmYFflypWVdd2JFM6cP39eO7ZkyRJlvVOnTtqeMmXUz5kKCgq0Pbqx2bNna3uef/557Rj+whM/AAAAQxD8AAAADEHwAwAAMATBDwAAwBAEPwAAAEMQ/AAAAAzh18e53Hzzzdqx6dOnK+u6I1vc6dixY8r6559/ru2ZMGGCsu7s83nppZeU9datW+snB3jQY489ph378ssvlfWsrCxtj+4N3Ytj8eLF2rHffvtNWa9Tp4625+GHH1bWR4wYYW9iMMbo0aOV9ZiYGNvXeuGFF7RjTz31lO3rwXvwxA8AAMAQBD8AAABDEPwAAAAMQfADAAAwBMEPAADAEH69q3fSpEnasejoaJfd5+eff9aO6Xbzff3119qe9PR0ZV2321dEJCgoSFl/6623tD3F2b2bm5urrB85csT2tQC7Dh8+rB3T7dQHTDF48GCXXWvhwoUuuxa8C0/8AAAADEHwAwAAMATBDwAAwBAEPwAAAEMQ/AAAAAxB8AMAADCEXxznUrt2bWW9YsWKLr2P7iiTBx98UNuje+P44ggNDdWOZWRkKOt33nmny+4vIjJ37lxlfdOmTS69DwDgUrrvdyIiISEhLruPs2PKwsPDlfWwsDBtz2233aasV65cWduzYcMGZX3p0qXaHlweT/wAAAAMQfADAAAwBMEPAADAEAQ/AAAAQxD8AAAADOEXu3qPHTumrJ85c8b2tb799lvt2LBhw5R1V+9oTUpKUtbHjh2r7bn22mtddv8DBw5oxx555BGX3QeA3qFDhzw9BXihLl26aMciIiJcdp8FCxZox3Q7i+vXr++y+4uInD9/XlkfOHCgtmf27NkunYM/4okfAACAIQh+AAAAhiD4AQAAGILgBwAAYAiCHwAAgCEIfgAAAIZwWJZlFekDHY7SnovLLVu2TDu2YsUKZX3y5MnanrNnz9qeQ6dOnZT1uXPnansiIyOV9TJlXJvTT548qaxfd9112p5t27a5dA6eVsS//m7li2sNIu3bt9eO6d5U/tSpU9qeJk2aKOs5OTn2JuYlWGv2BAcHK+s//fSTticuLs5l93d2tFmlSpWU9ezsbG1PnTp1lPWoqCh7ExP9MS8iItHR0cr6kSNHbN/HV11urfHEDwAAwBAEPwAAAEMQ/AAAAAxB8AMAADAEwQ8AAMAQgZ6eQGlKSkry9BQkOTlZWS9fvrybZ3KpgoICZf306dNungng+9q0aaMdCw8PV9bfe+89bY+v7t6Fa+Tn5yvrO3bs0PbodvU62wU7fvx4ZX3s2LHanrJlyyrrJ06c0PZUqVJFWb/11lu1Pa+++qqyvm/fPm1PZmamst6nTx9tz+7du7Vj/ognfgAAAIYg+AEAABiC4AcAAGAIgh8AAIAhCH4AAACGIPgBAAAYwq+Pc4FzuiNlHn/8cW3PI488oqzrjh6AGUJCQrRj7du3d9l9nB0X4exN5V3pyiuvVNaHDRtm+1q//fZbSacDP6U7bmvSpEnankOHDinrmzZt0va88sor9iYmztehTlZWlrL+zjvvaHtuuukmZb1bt27antjYWGX9ww8/1PY0a9ZMO+aPeOIHAABgCIIfAACAIQh+AAAAhiD4AQAAGILgBwAAYAiHZVlWkT7Q4SjtufilN954Q1kfPHiwtke3m2vgwIHantDQUGX92muv1fYMGDBAWY+MjNT26N5oOzs7W9vjzYr419+tvHmtBQcHK+uTJ0/W9tx7770uu39ubq52bMuWLcq6s12Q7733nu05jB49WlmfMGGCtufjjz9W1nv37q3tOXv2rL2JeTnWGuzSfV/bs2ePtke36/7MmTPanvj4eGV948aN+sl5scutNZ74AQAAGILgBwAAYAiCHwAAgCEIfgAAAIYg+AEAABiC4AcAAGAIjnPxkEqVKtnuycnJcekcEhISlPXPP/9c2/Pqq68q648++qhL5uRuHDFhzxNPPKGsjx8/XtujO4LF2VEqr7/+urJetWpVbc+oUaOU9ZYtW2p7du7cqazPmDFD2zNy5EhlvVatWtoe3Vpbs2aNtsffsNbgKk8++aR2zNm/RTr9+vVT1otz3JM34DgXAAAAiAjBDwAAwBgEPwAAAEMQ/AAAAAxB8AMAADAEu3oNVq1aNWX9999/1/b8+uuvynq9evVcMid3Y6fhpVq3bq0dW7p0qbIeHh6u7Rk0aJCyPm/ePHsTuwzdHHQ7akX0u3ejoqJs3z8zM1M71rNnT2X97Nmztu/jq1hrcJUmTZpox9atW6esh4aGanvmz5+vrN9+++32JuYl2NULAAAAESH4AQAAGIPgBwAAYAiCHwAAgCEIfgAAAIYg+AEAABgi0NMTgOfojnNxZv/+/aUwE3hCcHCwsv7JJ59oeypUqKCsHzhwQNvj6mNbdHJzc5X19evXa3uOHj2qrBfnOJfrr79eO9a5c2dlfcmSJbbvA5huy5Yt2jHdvwPOjnMxDU/8AAAADEHwAwAAMATBDwAAwBAEPwAAAEMQ/AAAAAzh17t6r7zySu1Yfn6+sn7kyJHSmo5HOHsz6w8++MD29ZYtW1aS6cCL6N6gXrdz15kpU6aUcDalp2/fvtqxevXq2b7eokWLlPXy5ctre2bPnq2sf/bZZ9qepUuXKuszZ87U9pw9e1Y7BvgLZ2stICDAjTPxTTzxAwAAMATBDwAAwBAEPwAAAEMQ/AAAAAxB8AMAADAEwQ8AAMAQfn2cy1dffaUdi4yMVNbT09O1PR9//LGyvm7dOnsTK6ZKlSppxx588EFl/b777tP2VKlSxfYcdu/ebbsH/u/48eNuuU9UVJR2bOHChcp6q1atbN+na9eu2rFPP/1UWQ8M1P9zqjt+YsSIEdqeV155RVl/9tlntT2vvfaasj5+/HhtD/yH7u+gs+NPbrzxRmVdd5yQiMjp06ftTayYdEdOOVs3zj5XnZ9//tl2jy/jiR8AAIAhCH4AAACGIPgBAAAYguAHAABgCIIfAACAIRyWZVlF+kDN7hpvtmPHDu1Y7dq1bV/v3Llzyvq+ffu0PfPnz7d9n1tvvVVZd7YLt1y5crbvo/t/unbtWm1Px44dlfW8vDzb9/cGRfzr71buWmshISHKem5uru1rvfvuu9qxQYMG2b5e5cqVlfWPPvpI26Pbvbt//35tT/fu3ZX1LVu2aHvy8/O1Y65UoUIFZX3UqFHaHt3adfZ1cxeT15q7NG/eXFlfv3697Wt9/vnn2jHd96iTJ0/avo8zw4cPV9YnTZpk+1pnz57Vjv3rX/9S1ovzdfMGl1trPPEDAAAwBMEPAADAEAQ/AAAAQxD8AAAADEHwAwAAMATBDwAAwBB+fZxLZmamdqxTp05unIl3WrdunbLu7A3qc3JySms6HmHyERO6+/Tu3Vvb8/777yvrzo5xmDBhgrKuO0pFRKRZs2bKenh4uLZnwYIFyvqYMWO0Paa9ObsnmbzW3KVOnTrK+oYNG7Q9ERERtu/z3XffKevFOf7E2b83V1xxhbJepoz9Z1bJycnasfHjx9u+njfjOBcAAACICMEPAADAGAQ/AAAAQxD8AAAADEHwAwAAMIRf7+oNCgrSjr300kvKep8+fbQ9UVFRJZ5TaSkoKFDWly9fru3p16+fsn7s2DFXTMknsNPwUiEhIdqxVatWKeutW7d26Ryys7OVdWc7dDMyMpR13dqAe7HWPGfKlCnasYEDByrrYWFhLp2D7mtdnL8Xzk6XePDBB5X1xYsXa3vOnj1rew7ejF29AAAAEBGCHwAAgDEIfgAAAIYg+AEAABiC4AcAAGAIgh8AAIAh/Po4l+KIiYnRjlWrVk1ZT0pK0vakpaWVdEqFvvzyS+3YvHnzlPU33njDZff3RxwxAbgHa807XX311cr6iBEjtD3XXHONrbqISFZWlrK+bNkybc/27duV9enTp2t7/vjjD+2YKTjOBQAAACJC8AMAADAGwQ8AAMAQBD8AAABDEPwAAAAMwa5eGI2dhoB7sNYA92BXLwAAAESE4AcAAGAMgh8AAIAhCH4AAACGIPgBAAAYguAHAABgCIIfAACAIQh+AAAAhiD4AQAAGILgBwAAYAiCHwAAgCEIfgAAAIYg+AEAABiC4AcAAGAIgh8AAIAhCH4AAACGIPgBAAAYguAHAABgCIIfAACAIRyWZVmengQAAABKH0/8AAAADEHwAwAAMATBDwAAwBAEPwAAAEMQ/AAAAAxB8AMAADAEwQ8AAMAQBD8AAABDEPwAAAAM8f8Aah89a5UbTz8AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing your data for training with DataLoaders"
      ],
      "metadata": {
        "id": "sc0XMO-GK3ZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=4, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=4, shuffle=True)"
      ],
      "metadata": {
        "id": "SsR1m9Pl9YK1"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iterate through the DataLoader"
      ],
      "metadata": {
        "id": "RHJsmIi2K7J2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display image and label.\n",
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")\n",
        "img = train_features[0].squeeze()\n",
        "label = train_labels[0]\n",
        "plt.imshow(img, cmap=\"gray\")\n",
        "plt.show()\n",
        "print(f\"Label: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "w8Y3LLC79ZsC",
        "outputId": "75571aa0-06b7-40fb-932f-b952acb527da"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature batch shape: torch.Size([4, 1, 28, 28])\n",
            "Labels batch shape: torch.Size([4])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the network"
      ],
      "metadata": {
        "id": "zO9BxIBkK9Ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(2): # loop over the dataset multiple times\n",
        "\n",
        "  running_loss = 0.0\n",
        "  for i, data in enumerate(train_dataloader, 0):\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "    inputs, labels = data\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = net(torch.flatten(inputs,1))\n",
        "    iteration_loss = loss(outputs, labels)\n",
        "    iteration_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # print statistics\n",
        "    running_loss += iteration_loss.item()\n",
        "    if i % 2000 == 1999: # print every 2000 mini-batches\n",
        "      print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "      running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ft-mnab9kGS",
        "outputId": "b06ed7d7-f2ac-4c60-f681-293edf986e28"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,  2000] loss: 2.274\n",
            "[1,  4000] loss: 2.152\n",
            "[1,  6000] loss: 1.805\n",
            "[1,  8000] loss: 1.238\n",
            "[1, 10000] loss: 0.856\n",
            "[1, 12000] loss: 0.668\n",
            "[1, 14000] loss: 0.576\n",
            "[2,  2000] loss: 0.481\n",
            "[2,  4000] loss: 0.454\n",
            "[2,  6000] loss: 0.432\n",
            "[2,  8000] loss: 0.422\n",
            "[2, 10000] loss: 0.385\n",
            "[2, 12000] loss: 0.376\n",
            "[2, 14000] loss: 0.379\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4: What is the meaning of epoch, forward pass, backward pass. What is the effect of torch.flatten(inputs, 1), and optimizer.step()?"
      ],
      "metadata": {
        "id": "udM_XNUNJ561"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch: An epoch is a single pass through the entire training dataset. The outer loop for epoch in range(2) indicates that the training process will loop over the dataset two times.\n",
        "\n",
        "Forward Pass: The forward pass is the process of passing input data through the neural network to compute the predicted outputs. In this code, outputs = net(torch.flatten(inputs, 1)) represents the forward pass, where inputs are passed through the neural network (net) after being flattened along the second dimension using torch.flatten(inputs, 1).\n",
        "\n",
        "Backward Pass: The backward pass, also known as backpropagation, is the process of computing gradients of the loss function with respect to the network's parameters. This is done using the backward() method: iteration_loss.backward(). It computes gradients for all the tensors used to compute iteration_loss.\n",
        "\n",
        "Optimizer: optimizer.step() updates the parameters of the neural network using the computed gradients and the optimization algorithm (e.g., SGD, Adam) to minimize the loss. It adjusts the network's weights and biases based on the computed gradients and the specified optimization strategy.\n",
        "\n",
        "torch.flatten(inputs, 1): This function reshapes the input tensor inputs to have a flattened shape along the second dimension (dimension 1). It's likely used to prepare the input data for the neural network if it expects a flattened representation."
      ],
      "metadata": {
        "id": "PSJygMBNJ59p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = './my_net.pth'\n",
        "torch.save(net.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "edqjMAUyJ_1M"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the network on the test data"
      ],
      "metadata": {
        "id": "3bNr6fNpLBgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net()\n",
        "net.load_state_dict(torch.load(PATH))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUYTvktsKAt9",
        "outputId": "d0757668-833c-4019-cc63-310bf7b9c8c7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "  for data in test_dataloader:\n",
        "    images, labels = data\n",
        "    # calculate outputs by running images through the network\n",
        "    outputs = net(torch.flatten(images,1))\n",
        "    # the class with the highest energy is what we choose as prediction\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct// total} %')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAO8dtv4KDbZ",
        "outputId": "87de5562-9c59-48ea-b549-3d4fe1a41f5a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 89 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 5: Train the network in the previous example, but instead of using 2 hidden layers, try 3 hidden layers."
      ],
      "metadata": {
        "id": "rNHGyEwjKSPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Assuming net is the previously defined neural network with 2 hidden layers\n",
        "\n",
        "class ThreeHiddenLayerNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ThreeHiddenLayerNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 64)  # First hidden layer\n",
        "        self.fc2 = nn.Linear(64, 64)   # Second hidden layer\n",
        "        self.fc3 = nn.Linear(64, 64)   # Third hidden layer\n",
        "        self.fc4 = nn.Linear(64, 10)   # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = torch.relu(self.fc1(x))   # ReLU activation for first hidden layer\n",
        "        x = torch.relu(self.fc2(x))   # ReLU activation for second hidden layer\n",
        "        x = torch.relu(self.fc3(x))   # ReLU activation for third hidden layer\n",
        "        x = self.fc4(x)               # Output layer without activation (for example, using CrossEntropyLoss)\n",
        "        return x"
      ],
      "metadata": {
        "id": "-g107n19Q2Je"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the network\n",
        "net = ThreeHiddenLayerNet()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(2): # loop over the dataset multiple times\n",
        "\n",
        "  running_loss = 0.0\n",
        "  for i, data in enumerate(train_dataloader, 0):\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "    inputs, labels = data\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = net(torch.flatten(inputs,1))\n",
        "    iteration_loss = loss(outputs, labels)\n",
        "    iteration_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # print statistics\n",
        "    running_loss += iteration_loss.item()\n",
        "    if i % 2000 == 1999: # print every 2000 mini-batches\n",
        "      print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "      running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "  for data in test_dataloader:\n",
        "    images, labels = data\n",
        "    # calculate outputs by running images through the network\n",
        "    outputs = net(torch.flatten(images,1))\n",
        "    # the class with the highest energy is what we choose as prediction\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct// total} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuuNNRQzqrcK",
        "outputId": "d98536b3-c780-456d-d041-6baf1d709330"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,  2000] loss: 2.295\n",
            "[1,  4000] loss: 2.269\n",
            "[1,  6000] loss: 2.214\n",
            "[1,  8000] loss: 2.083\n",
            "[1, 10000] loss: 1.746\n",
            "[1, 12000] loss: 1.232\n",
            "[1, 14000] loss: 0.897\n",
            "[2,  2000] loss: 0.693\n",
            "[2,  4000] loss: 0.590\n",
            "[2,  6000] loss: 0.532\n",
            "[2,  8000] loss: 0.511\n",
            "[2, 10000] loss: 0.472\n",
            "[2, 12000] loss: 0.449\n",
            "[2, 14000] loss: 0.447\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 87 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 6: Train the network in the previous example using Adam optimizer"
      ],
      "metadata": {
        "id": "EgSQEc66KTI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the network\n",
        "net = ThreeHiddenLayerNet()\n",
        "\n",
        "# Use Adam optimizer with a learning rate of 0.001\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# Assuming train_dataloader contains your training dataset\n",
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_dataloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        iteration_loss = loss(outputs, labels)\n",
        "        iteration_loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += iteration_loss.item()\n",
        "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "  for data in test_dataloader:\n",
        "    images, labels = data\n",
        "    # calculate outputs by running images through the network\n",
        "    outputs = net(torch.flatten(images,1))\n",
        "    # the class with the highest energy is what we choose as prediction\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct// total} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTBAqk5uKUpI",
        "outputId": "e54e5b50-6fe2-48e3-b596-273a25c5e91f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,  2000] loss: 0.582\n",
            "[1,  4000] loss: 0.282\n",
            "[1,  6000] loss: 0.233\n",
            "[1,  8000] loss: 0.210\n",
            "[1, 10000] loss: 0.185\n",
            "[1, 12000] loss: 0.174\n",
            "[1, 14000] loss: 0.171\n",
            "[2,  2000] loss: 0.138\n",
            "[2,  4000] loss: 0.138\n",
            "[2,  6000] loss: 0.131\n",
            "[2,  8000] loss: 0.139\n",
            "[2, 10000] loss: 0.127\n",
            "[2, 12000] loss: 0.136\n",
            "[2, 14000] loss: 0.116\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 96 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training on GPU"
      ],
      "metadata": {
        "id": "Pjxa4_pvLFgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXl9R15CKWlp",
        "outputId": "ae7f749a-35df-4b42-e2ed-43a5169280ac"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net()\n",
        "\n",
        "net.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTRdWzO7KZFK",
        "outputId": "dc792378-e8be-4347-c025-1b10900c26ab"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
              "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
              "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 7: Train the network in the previous example on GPU. Do you notice significant speedup? if not, try to increase the size of your network."
      ],
      "metadata": {
        "id": "s6v1S2VYKdXV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we try on small network."
      ],
      "metadata": {
        "id": "TLqnx02fwbDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(2): # loop over the dataset multiple times\n",
        "\n",
        "  running_loss = 0.0\n",
        "  for i, data in enumerate(train_dataloader, 0):\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "    inputs, labels = data[0].to(device), data[1].to(device)\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = net(torch.flatten(inputs,1))\n",
        "    iteration_loss = loss(outputs, labels)\n",
        "    iteration_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # print statistics\n",
        "    running_loss += iteration_loss.item()\n",
        "    if i % 2000 == 1999: # print every 2000 mini-batches\n",
        "      print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "      running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmqLlejeKbEh",
        "outputId": "8c06cba1-b252-49fc-8d04-ae632938ad23"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,  2000] loss: 2.269\n",
            "[1,  4000] loss: 2.127\n",
            "[1,  6000] loss: 1.775\n",
            "[1,  8000] loss: 1.300\n",
            "[1, 10000] loss: 0.952\n",
            "[1, 12000] loss: 0.761\n",
            "[1, 14000] loss: 0.631\n",
            "[2,  2000] loss: 0.529\n",
            "[2,  4000] loss: 0.482\n",
            "[2,  6000] loss: 0.475\n",
            "[2,  8000] loss: 0.436\n",
            "[2, 10000] loss: 0.410\n",
            "[2, 12000] loss: 0.408\n",
            "[2, 14000] loss: 0.395\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CPU time: 46s\n",
        "\n",
        "GPU time: 56s\n",
        "\n",
        "We can't see the speedup, even CPU is faster. So, we will increase the size of the network as follows:"
      ],
      "metadata": {
        "id": "ytXAdevch2ow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LargerThreeHiddenLayerNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LargerThreeHiddenLayerNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 512)  # First hidden layer with 128 neurons\n",
        "        self.fc2 = nn.Linear(512, 512)  # Second hidden layer with 128 neurons\n",
        "        self.fc3 = nn.Linear(512, 10)   # Output layer with 10 neurons\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = torch.relu(self.fc1(x))   # ReLU activation for first hidden layer\n",
        "        x = torch.relu(self.fc2(x))   # ReLU activation for second hidden layer\n",
        "        x = self.fc3(x)               # Output layer without activation (for example, using CrossEntropyLoss)\n",
        "        return x"
      ],
      "metadata": {
        "id": "nXfcNnASwvAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Assuming net is the previously defined neural network with 3 hidden layers\n",
        "\n",
        "# Check if GPU is available and set the device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Training on {device}\")\n",
        "\n",
        "# Initialize the larger network and move it to the device (GPU if available)\n",
        "net = LargerThreeHiddenLayerNet().to(device)\n",
        "\n",
        "# Define the loss function\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# Use Adam optimizer with a learning rate of 0.001\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# Assuming train_dataloader contains your training dataset\n",
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_dataloader, 0):\n",
        "        inputs, labels = data\n",
        "        # Move the inputs and labels to the device\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        iteration_loss = loss(outputs, labels)\n",
        "        iteration_loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += iteration_loss.item()\n",
        "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6vOusnXmE0s",
        "outputId": "ec1cadac-5687-4018-e91c-840a8f21f096"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on cuda:0\n",
            "[1,  2000] loss: 0.420\n",
            "[1,  4000] loss: 0.239\n",
            "[1,  6000] loss: 0.195\n",
            "[1,  8000] loss: 0.183\n",
            "[1, 10000] loss: 0.167\n",
            "[1, 12000] loss: 0.157\n",
            "[1, 14000] loss: 0.152\n",
            "[2,  2000] loss: 0.116\n",
            "[2,  4000] loss: 0.117\n",
            "[2,  6000] loss: 0.125\n",
            "[2,  8000] loss: 0.111\n",
            "[2, 10000] loss: 0.129\n",
            "[2, 12000] loss: 0.110\n",
            "[2, 14000] loss: 0.103\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Assuming net is the previously defined neural network with 3 hidden layers\n",
        "\n",
        "# Check if GPU is available and set the device\n",
        "device = torch.device(\"cpu\")\n",
        "print(f\"Training on {device}\")\n",
        "\n",
        "# Initialize the larger network and move it to the device (GPU if available)\n",
        "net = LargerThreeHiddenLayerNet().to(device)\n",
        "\n",
        "# Define the loss function\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# Use Adam optimizer with a learning rate of 0.001\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# Assuming train_dataloader contains your training dataset\n",
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_dataloader, 0):\n",
        "        inputs, labels = data\n",
        "        # Move the inputs and labels to the device\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        iteration_loss = loss(outputs, labels)\n",
        "        iteration_loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += iteration_loss.item()\n",
        "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwGlLoW7vhyx",
        "outputId": "8fe31e6b-fd9e-4c4d-d9cc-5c2034dc6e8a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on cpu\n",
            "[1,  2000] loss: 0.425\n",
            "[1,  4000] loss: 0.249\n",
            "[1,  6000] loss: 0.205\n",
            "[1,  8000] loss: 0.186\n",
            "[1, 10000] loss: 0.165\n",
            "[1, 12000] loss: 0.146\n",
            "[1, 14000] loss: 0.137\n",
            "[2,  2000] loss: 0.108\n",
            "[2,  4000] loss: 0.129\n",
            "[2,  6000] loss: 0.122\n",
            "[2,  8000] loss: 0.114\n",
            "[2, 10000] loss: 0.116\n",
            "[2, 12000] loss: 0.113\n",
            "[2, 14000] loss: 0.103\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPU time: 1m 17s\n",
        "\n",
        "CPU time: 7m 28s\n",
        "\n",
        "Now, we can see the huge difference between the run on CPU and GPU"
      ],
      "metadata": {
        "id": "91PK3UwYtYEU"
      }
    }
  ]
}